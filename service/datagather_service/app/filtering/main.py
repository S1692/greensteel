import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from typing import Dict, List, Optional

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# AI ëª¨ë¸ import
try:
    import sys
    import os
    
    # í˜„ì¬ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ë””ë ‰í† ë¦¬ë“¤ì„ sys.pathì— ì¶”ê°€
    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)  # app/
    grandparent_dir = os.path.dirname(parent_dir)  # appì˜ ìƒìœ„
    
    if grandparent_dir not in sys.path:
        sys.path.insert(0, grandparent_dir)
    
    # GPU ì„¤ì •ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    import os
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    from app.ananke.model import XMLRoBERTaClassifier
    AI_AVAILABLE = True
    print("AI ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤.")
except ImportError as e:
    AI_AVAILABLE = False
    print(f"AI ëª¨ë¸ import ì‹¤íŒ¨: {e}")
    print("ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def predict_material_name(input_material: str, process: str, production_name: str, ai_model=None) -> str:
    """
    AI ëª¨ë¸ ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ íˆ¬ì…ë¬¼ëª…ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    
    Args:
        input_material: ì›ë³¸ íˆ¬ì…ë¬¼ëª…
        process: ê³µì •ëª…
        production_name: ìƒì‚°í’ˆëª…
        ai_model: AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
    
    Returns:
        ìˆ˜ì •ëœ íˆ¬ì…ë¬¼ëª…
    """
    try:
        # ì…ë ¥ê°’ ê²€ì¦ - ìƒíƒœê°’ì´ ì•„ë‹Œ ì‹¤ì œ íˆ¬ì…ë¬¼ëª…ì¸ì§€ í™•ì¸
        if not input_material or input_material.strip() == '':
            return ''
        
        # ìƒíƒœê°’ìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒë“¤ì€ ì œì™¸
        status_values = ['ì™„ë£Œ', 'ì§„í–‰ì¤‘', 'ëŒ€ê¸°', 'ì·¨ì†Œ', 'ë³´ë¥˜', 'ì¢…ë£Œ']
        if input_material.strip() in status_values:
            logger.warning(f"ìƒíƒœê°’ì´ íˆ¬ì…ë¬¼ëª…ìœ¼ë¡œ ì…ë ¥ë¨: {input_material}")
            return input_material  # ìƒíƒœê°’ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        # AI ëª¨ë¸ í•„ìˆ˜ ì‚¬ìš© - ê·œì¹™ ê¸°ë°˜ ì œê±°
        if AI_AVAILABLE and ai_model:
            try:
                # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ ë° ìµœì í™”
                import torch
                if torch.cuda.is_available():
                    try:
                        torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                        logger.info(f"ğŸš€ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
                    except Exception as gpu_error:
                        logger.warning(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {gpu_error}")
                
                # AI ëª¨ë¸ì„ í†µí•œ ì˜ˆì¸¡ (í•™ìŠµëœ ë¼ë²¨ë§Œ ì‚¬ìš©)
                logger.info(f"ğŸ¤– AI ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘: '{input_material}'")
                
                # ë¶„ë¥˜ê¸°ì™€ ë ˆì´ë¸” ì„ë² ë”© ìƒíƒœ í™•ì¸
                if not (hasattr(ai_model, 'classifier') and ai_model.classifier is not None):
                    raise Exception("ë¶„ë¥˜ê¸°ê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                
                if not (hasattr(ai_model, 'label_embeddings') and ai_model.label_embeddings):
                    raise Exception("ë ˆì´ë¸” ì„ë² ë”©ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
                
                logger.info(f"âœ… AI ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ - ë¶„ë¥˜ê¸°: ì •ìƒ, ë ˆì´ë¸”: {len(ai_model.label_embeddings)}ê°œ")
                
                # AI ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ (í•™ìŠµëœ ë¼ë²¨ë§Œ ë°˜í™˜)
                ai_prediction_results = ai_model.predict(input_material)
                logger.info(f"AI ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼: {ai_prediction_results}")
                
                if ai_prediction_results and len(ai_prediction_results) > 0:
                    # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ í•™ìŠµëœ ë¼ë²¨ ì‚¬ìš©
                    best_prediction = ai_prediction_results[0]['label']
                    confidence = ai_prediction_results[0]['similarity']
                    
                    # ì‹ ë¢°ë„ ì„ê³„ê°’ í™•ì¸ (30% ì´ìƒë§Œ ì‹ ë¢°)
                    if confidence >= 30.0:
                        logger.info(f"ğŸ¯ AI ëª¨ë¸ ì˜ˆì¸¡ ì„±ê³µ: {input_material} â†’ {best_prediction} (ì‹ ë¢°ë„: {confidence:.1f}%)")
                        return best_prediction
                    else:
                        logger.warning(f"âš ï¸ AI ëª¨ë¸ ì‹ ë¢°ë„ ë‚®ìŒ: {input_material} â†’ {best_prediction} (ì‹ ë¢°ë„: {confidence:.1f}% < 30%)")
                        # ë‚®ì€ ì‹ ë¢°ë„ë¼ë„ í•™ìŠµëœ ë¼ë²¨ ì¤‘ì—ì„œ ìµœì„ ì˜ ì„ íƒ ë°˜í™˜
                        return best_prediction
                else:
                    logger.warning(f"âŒ AI ëª¨ë¸ì´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ: {input_material}")
                    # ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
                    return input_material
                    
            except Exception as ai_error:
                logger.error(f"âŒ AI ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {ai_error}")
                # AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œì—ë„ ì›ë³¸ ë°˜í™˜ (ê·œì¹™ ê¸°ë°˜ ì‚¬ìš© ì•ˆ í•¨)
                logger.info(f"AI ëª¨ë¸ ì‹¤íŒ¨ë¡œ ì›ë³¸ ë°˜í™˜: {input_material}")
                return input_material
        else:
            # AI ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë°˜í™˜ (ê·œì¹™ ê¸°ë°˜ ì‚¬ìš© ì•ˆ í•¨)
            logger.warning(f"AI ëª¨ë¸ì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•˜ì—¬ ì›ë³¸ ë°˜í™˜: {input_material}")
            return input_material
        
    except Exception as e:
        logger.error(f"íˆ¬ì…ë¬¼ëª… ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
        return input_material

# í•˜ë“œì½”ë”©ëœ ê·œì¹™ ê¸°ë°˜ í•¨ìˆ˜ë“¤ì„ ì œê±°í•˜ê³  AI ëª¨ë¸ë§Œ ì‚¬ìš©

async def save_feedback_to_training_data(feedback_data: dict):
    """í”¼ë“œë°± ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„° íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        # í•™ìŠµ ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
        current_dir = os.path.dirname(os.path.abspath(__file__))
        data_dir = os.path.join(current_dir, "..", "data", "dataforstudy")
        feedback_file = os.path.join(data_dir, "feedback_data.jsonl")
        
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(data_dir, exist_ok=True)
        
        # í”¼ë“œë°± ë°ì´í„°ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
        with open(feedback_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(feedback_data, ensure_ascii=False) + '\n')
        
        logger.info(f"í”¼ë“œë°± ë°ì´í„°ê°€ í•™ìŠµ ë°ì´í„° íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {feedback_file}")
        
    except Exception as e:
        logger.error(f"í”¼ë“œë°± ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    logger.info(f"DataGather Service starting up...")
    logger.info("Domain: Data Collection & Processing")
    logger.info(f"Architecture: AI Model + Rule-based (AI Available: {AI_AVAILABLE})")
    
    # AI ëª¨ë¸ ì´ˆê¸°í™”
    if AI_AVAILABLE:
        try:
            # GPU ì„¤ì • ìµœì í™”
            import torch
            if torch.cuda.is_available():
                # CUDA ë©”ëª¨ë¦¬ í• ë‹¹ì ì„¤ì •
                torch.cuda.set_per_process_memory_fraction(0.8)  # GPU ë©”ëª¨ë¦¬ì˜ 80%ë§Œ ì‚¬ìš©
                logger.info(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
                logger.info(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            else:
                logger.info("GPU ì‚¬ìš© ë¶ˆê°€ëŠ¥, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            
            # ìƒˆë¡œ ì¶”ê°€ëœ model_v24 í•™ìŠµ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
            current_dir = os.path.dirname(os.path.abspath(__file__))
            model_dir = os.path.join(current_dir, "..", "data", "studied", "model_v24", "model_v24")
            
            # AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìƒˆë¡œ ì¶”ê°€ëœ model_v24 í•™ìŠµ ëª¨ë¸ ì‚¬ìš©)
            try:
                logger.info(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ í™•ì¸: {model_dir}")
                logger.info(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(model_dir)}")
                
                if os.path.exists(model_dir):
                    # ëª¨ë¸ ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
                    model_files = os.listdir(model_dir)
                    logger.info(f"ëª¨ë¸ ë””ë ‰í† ë¦¬ ë‚´ìš©: {model_files}")
                    
                    # í•„ìˆ˜ íŒŒì¼ë“¤ í™•ì¸
                    required_files = ['config.json', 'pytorch_model.bin', 'tokenizer.json', 'classifier.pkl', 'label_mapping.json']
                    missing_files = [f for f in required_files if f not in model_files]
                    if missing_files:
                        logger.warning(f"ëˆ„ë½ëœ ëª¨ë¸ íŒŒì¼ë“¤: {missing_files}")
                    
                    logger.info(f"ìƒˆë¡œ ì¶”ê°€ëœ model_v24 í•™ìŠµ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {model_dir}")
                    app.state.ai_model = XMLRoBERTaClassifier(model_dir=model_dir)
                    logger.info("model_v24 í•™ìŠµ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    logger.info("AI ëª¨ë¸: ìµœì‹  í•™ìŠµëœ ë°ì´í„°(v24)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì˜ˆì¸¡ ìˆ˜í–‰")
                    
                    # ëª¨ë¸ ë¡œë“œ ì„±ê³µ í™•ì¸
                    if hasattr(app.state.ai_model, 'classifier') and app.state.ai_model.classifier is not None:
                        logger.info("âœ… AI ëª¨ë¸ì˜ ë¶„ë¥˜ê¸°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        logger.warning("âš ï¸ AI ëª¨ë¸ì˜ ë¶„ë¥˜ê¸° ë¡œë“œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
                        
                    if hasattr(app.state.ai_model, 'label_embeddings') and app.state.ai_model.label_embeddings:
                        logger.info(f"âœ… ë ˆì´ë¸” ì„ë² ë”©ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {len(app.state.ai_model.label_embeddings)}ê°œ")
                    else:
                        logger.warning("âš ï¸ ë ˆì´ë¸” ì„ë² ë”© ë¡œë“œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
                else:
                    logger.warning(f"model_v24 ëª¨ë¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_dir}")
                    # fallback: latest_model í´ë” í™•ì¸
                    fallback_dir = os.path.join(current_dir, "..", "data", "studied", "latest_model")
                    if os.path.exists(fallback_dir):
                        logger.info(f"fallback: ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {fallback_dir}")
                        app.state.ai_model = XMLRoBERTaClassifier(model_dir=fallback_dir)
                        logger.info("ê¸°ì¡´ í•™ìŠµëœ AI ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        logger.info("í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                        app.state.ai_model = XMLRoBERTaClassifier()
                        logger.info("ìƒˆë¡œìš´ AI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        logger.info("AI ëª¨ë¸: ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡")
                    
            except Exception as model_error:
                import traceback
                logger.error(f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {model_error}")
                logger.error(f"ì—ëŸ¬ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
                logger.info("ê·œì¹™ ê¸°ë°˜ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                app.state.ai_model = None
                
        except Exception as e:
            logger.error(f"AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            app.state.ai_model = None
    else:
        app.state.ai_model = None
        logger.info("AI ëª¨ë¸ ì—†ì´ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    
    yield
    # ì¢…ë£Œ ì‹œ
    logger.info(f"DataGather Service shutting down...")

def create_app() -> FastAPI:
    """FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ íŒ©í† ë¦¬"""
    
    # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    app = FastAPI(
        title="DataGather Service - AI Model + Rule-based",
        description="ESG ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì„œë¹„ìŠ¤ - AI ëª¨ë¸ê³¼ ê·œì¹™ ê¸°ë°˜ í˜¼í•©",
        version="1.0.0",
        lifespan=lifespan
    )
    
    # CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["http://localhost:3000", "http://localhost:8080"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"]
    )
    
    # í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
    @app.get("/health")
    async def health_check():
        """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
        # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = "None"
        if hasattr(app.state, 'ai_model') and app.state.ai_model:
            model_info = "model_v24 (ìµœì‹  í•™ìŠµ ëª¨ë¸)"
        elif AI_AVAILABLE:
            model_info = "ê¸°ë³¸ ëª¨ë¸ (fallback)"
        
        return {
            "status": "ok",
            "service": "datagather",
            "domain": "data-collection",
            "architecture": "AI Model + Rule-based",
            "ai_available": AI_AVAILABLE,
            "current_model": model_info,
            "model_version": "v24",
            "timestamp": datetime.now().isoformat(),
            "version": "1.0.0"
        }
    
    # ë£¨íŠ¸ ê²½ë¡œ
    @app.get("/")
    async def root():
        """ë£¨íŠ¸ ê²½ë¡œ"""
        # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = "None"
        if hasattr(app.state, 'ai_model') and app.state.ai_model:
            model_info = "model_v24 (ìµœì‹  í•™ìŠµ ëª¨ë¸)"
        elif AI_AVAILABLE:
            model_info = "ê¸°ë³¸ ëª¨ë¸ (fallback)"
        
        return {
            "service": "DataGather Service",
            "version": "1.0.0",
            "domain": "Data Collection & Processing",
            "architecture": "AI Model + Rule-based",
            "ai_available": AI_AVAILABLE,
            "current_model": model_info,
            "model_version": "v24",
            "endpoints": {
                "health": "/health",
                "ai-process": "/ai-process",
                "feedback": "/feedback"
            }
        }
    
    # AI ëª¨ë¸ì„ í™œìš©í•œ íˆ¬ì…ë¬¼ëª… ìˆ˜ì • ì—”ë“œí¬ì¸íŠ¸
    @app.post("/ai-process")
    async def ai_process_data(data: dict):
        """AI ëª¨ë¸ê³¼ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ íˆ¬ì…ë¬¼ëª…ì„ ìë™ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"íˆ¬ì…ë¬¼ëª… ìˆ˜ì • ìš”ì²­ ë°›ìŒ: {data.get('filename', 'unknown')}")
            
            # ë°ì´í„° êµ¬ì¡° ê²€ì¦
            excel_data = data.get('data', [])
            if not excel_data:
                raise HTTPException(status_code=400, detail="ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            
            # ì˜ˆìƒ ì»¬ëŸ¼ëª… ì •ì˜ (í’ˆë²ˆ, ìˆ˜ëŸ‰ì€ ì„ íƒì‚¬í•­ìœ¼ë¡œ ë³€ê²½)
            required_columns = ['ìƒíƒœ', 'ë¡œíŠ¸ë²ˆí˜¸', 'ìƒì‚°í’ˆëª…', 'íˆ¬ì…ì¼', 'ì¢…ë£Œì¼', 'ê³µì •', 'íˆ¬ì…ë¬¼ëª…', 'ì§€ì‹œë²ˆí˜¸']
            optional_columns = ['í’ˆë²ˆ', 'ìˆ˜ëŸ‰']
            expected_columns = required_columns + optional_columns
            
            # ì²« ë²ˆì§¸ í–‰ì˜ ì»¬ëŸ¼ í™•ì¸
            first_row = excel_data[0] if excel_data else {}
            actual_columns = list(first_row.keys())
            
            # í•„ìˆ˜ ì»¬ëŸ¼ë§Œ ê²€ì¦ (í’ˆë²ˆ, ìˆ˜ëŸ‰ì€ ì„ íƒì‚¬í•­)
            missing_required_columns = [col for col in required_columns if col not in actual_columns]
            if missing_required_columns:
                raise HTTPException(
                    status_code=400, 
                    detail=f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_required_columns}"
                )
            
            # ì„ íƒì‚¬í•­ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ë¹ˆ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            for row in excel_data:
                for optional_col in optional_columns:
                    if optional_col not in row:
                        row[optional_col] = ''
            
            # AI ëª¨ë¸ê³¼ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ íˆ¬ì…ë¬¼ëª… ìˆ˜ì •
            processed_data = []
            for i, row in enumerate(excel_data):
                try:
                    # íˆ¬ì…ë¬¼ëª…ê³¼ ê³µì • ì¶”ì¶œ
                    input_material = row.get('íˆ¬ì…ë¬¼ëª…', '')
                    process = row.get('ê³µì •', '')
                    production_name = row.get('ìƒì‚°í’ˆëª…', '')
                    
                    if input_material and process:
                        # AI ëª¨ë¸ ë˜ëŠ” ê·œì¹™ ê¸°ë°˜ íˆ¬ì…ë¬¼ëª… ìˆ˜ì •
                        ai_model = getattr(app.state, 'ai_model', None)
                        corrected_material = predict_material_name(
                            input_material, 
                            process, 
                            production_name,
                            ai_model
                        )
                        
                        method_used = "AI ëª¨ë¸" if ai_model and AI_AVAILABLE else "ê·œì¹™ ê¸°ë°˜"
                        if ai_model and AI_AVAILABLE:
                            logger.info(f"ğŸ¤– AI ëª¨ë¸ ìˆ˜ì •: {input_material} â†’ {corrected_material}")
                            logger.info(f"ğŸ§  AI ëª¨ë¸: í•™ìŠµëœ ë¼ë²¨ ê¸°ë°˜ ì˜ˆì¸¡ (ê·œì¹™ ì—†ìŒ)")
                            # ë¶„ë¥˜ ì •í™•ë„ ëª¨ë‹ˆí„°ë§
                            if input_material != corrected_material:
                                logger.info(f"âœ… AI ëª¨ë¸ì´ íˆ¬ì…ë¬¼ëª…ì„ í•™ìŠµëœ ë¼ë²¨ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤")
                            else:
                                logger.info(f"â„¹ï¸ AI ëª¨ë¸ì´ ì›ë³¸ íˆ¬ì…ë¬¼ëª…ì„ ìµœì  ë¼ë²¨ë¡œ íŒë‹¨í–ˆìŠµë‹ˆë‹¤")
                        else:
                            logger.info(f"âš ï¸ AI ëª¨ë¸ ë¯¸ì‚¬ìš©: {input_material} â†’ {corrected_material}")
                        logger.info(f"ğŸ“‹ ê³µì • ì •ë³´: {process} (ì°¸ê³ ìš©, AI ì˜ˆì¸¡ì—ëŠ” ë¯¸ì‚¬ìš©)")
                        
                        # ìˆ˜ì •ëœ í–‰ ìƒì„±
                        processed_row = row.copy()
                        processed_row['íˆ¬ì…ë¬¼ëª…ìˆ˜ì •'] = corrected_material
                        processed_row['ai_processed'] = ai_model is not None and AI_AVAILABLE
                        processed_row['row_index'] = i
                        processed_row['method'] = method_used
                        
                        processed_data.append(processed_row)
                    else:
                        # íˆ¬ì…ë¬¼ëª…ì´ë‚˜ ê³µì •ì´ ì—†ëŠ” ê²½ìš° ì›ë³¸ ê·¸ëŒ€ë¡œ
                        processed_row = row.copy()
                        processed_row['íˆ¬ì…ë¬¼ëª…ìˆ˜ì •'] = input_material
                        processed_row['ai_processed'] = False
                        processed_row['row_index'] = i
                        processed_row['method'] = "ì›ë³¸ ìœ ì§€"
                        processed_data.append(processed_row)
                        
                except Exception as e:
                    logger.error(f"í–‰ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    processed_row = row.copy()
                    processed_row['íˆ¬ì…ë¬¼ëª…ìˆ˜ì •'] = row.get('íˆ¬ì…ë¬¼ëª…', '')
                    processed_row['ai_processed'] = False
                    processed_row['row_index'] = i
                    processed_row['error'] = str(e)
                    processed_row['method'] = "ì˜¤ë¥˜"
                    processed_data.append(processed_row)
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                "status": "processed",
                "message": f"ğŸ¤– AI ëª¨ë¸(v24)ë¡œ íˆ¬ì…ë¬¼ëª…ì„ í•™ìŠµëœ ë¼ë²¨ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤ (GPU ê°€ì†: {torch.cuda.is_available() if 'torch' in locals() else False})",
                "filename": data.get('filename'),
                "original_count": len(excel_data),
                "processed_count": len(processed_data),
                "ai_available": AI_AVAILABLE,
                "model_version": "v24",
                "model_type": "í•™ìŠµëœ ë¼ë²¨ ê¸°ë°˜ (ê·œì¹™ ì—†ìŒ)",
                "current_model": "model_v24 (ìµœì‹  í•™ìŠµ ëª¨ë¸)" if hasattr(app.state, 'ai_model') and app.state.ai_model else "AI ëª¨ë¸ ì—†ìŒ",
                "gpu_enabled": torch.cuda.is_available() if 'torch' in locals() else False,
                "data": processed_data,
                "columns": expected_columns + ['íˆ¬ì…ë¬¼ëª…ìˆ˜ì •'],
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"íˆ¬ì…ë¬¼ëª… ìˆ˜ì • ì™„ë£Œ: {len(processed_data)}í–‰ ì²˜ë¦¬ë¨")
            return result
            
        except Exception as e:
            logger.error(f"íˆ¬ì…ë¬¼ëª… ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise HTTPException(status_code=500, detail=f"íˆ¬ì…ë¬¼ëª… ìˆ˜ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    # ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
    @app.post("/feedback")
    async def process_feedback(feedback_data: dict):
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ì•„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            logger.info(f"ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬ ìš”ì²­ ë°›ìŒ")
            
            # í”¼ë“œë°± ë°ì´í„° ê²€ì¦
            row_index = feedback_data.get('row_index')
            original_material = feedback_data.get('original_material')
            corrected_material = feedback_data.get('corrected_material')
            reason = feedback_data.get('reason', '')
            production_name = feedback_data.get('production_name', '')
            process = feedback_data.get('process', '')
            
            if not all([row_index is not None, original_material, corrected_material]):
                raise HTTPException(status_code=400, detail="í•„ìˆ˜ í”¼ë“œë°± ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # í”¼ë“œë°± ë°ì´í„° ë¡œê¹… (í–¥í›„ AI ëª¨ë¸ í•™ìŠµì— í™œìš© ê°€ëŠ¥)
            logger.info(f"í”¼ë“œë°± ë°ì´í„°: {original_material} â†’ {corrected_material}")
            logger.info(f"ë§¥ë½: ìƒì‚°í’ˆëª…={production_name}, ê³µì •={process}, ì‚¬ìœ ={reason}")
            
            # AI ëª¨ë¸ì´ ìˆëŠ” ê²½ìš° í”¼ë“œë°± ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì „ë‹¬
            if AI_AVAILABLE and hasattr(app.state, 'ai_model') and app.state.ai_model:
                try:
                    logger.info("AI ëª¨ë¸ì— í”¼ë“œë°± ë°ì´í„°ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.")
                    logger.info(f"í•™ìŠµ ë°ì´í„°: ì›ë³¸={original_material}, ìˆ˜ì •={corrected_material}")
                    logger.info(f"ë§¥ë½ ì •ë³´: ìƒì‚°í’ˆëª…={production_name}, ê³µì •={process}, ì‚¬ìœ ={reason}")
                    
                    # AI ëª¨ë¸ì— ì¶”ê°€ í•™ìŠµ ë°ì´í„° ì „ë‹¬
                    # ê³µì •ê³¼ í”¼ë“œë°± ë°ì´í„°ë¥¼ í•¨ê»˜ í•™ìŠµí•˜ì—¬ í–¥í›„ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ
                    learning_context = {
                        'original_material': original_material,
                        'corrected_material': corrected_material,
                        'production_name': production_name,
                        'process': process,
                        'reason': reason,
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    # í”¼ë“œë°± ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„° íŒŒì¼ì— ì €ì¥
                    await save_feedback_to_training_data(learning_context)
                    
                    # ì‹¤ì œ AI ëª¨ë¸ í•™ìŠµ ë©”ì„œë“œ í˜¸ì¶œ (í”Œë ˆì´ìŠ¤í™€ë”)
                    # app.state.ai_model.learn(learning_context)
                    logger.info("AI ëª¨ë¸ì´ ê³µì •ê³¼ í”¼ë“œë°± ë°ì´í„°ë¥¼ í•¨ê»˜ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.")
                    
                except Exception as e:
                    logger.warning(f"AI ëª¨ë¸ í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            else:
                logger.info("AI ëª¨ë¸ì´ ì—†ì–´ í”¼ë“œë°± ë°ì´í„°ë¥¼ ì €ì¥ë§Œ í•©ë‹ˆë‹¤.")
                # AI ëª¨ë¸ì´ ì—†ì–´ë„ í”¼ë“œë°± ë°ì´í„°ëŠ” ì €ì¥
                learning_context = {
                    'original_material': original_material,
                    'corrected_material': corrected_material,
                    'production_name': production_name,
                    'process': process,
                    'reason': reason,
                    'timestamp': datetime.now().isoformat()
                }
                await save_feedback_to_training_data(learning_context)
            
            result = {
                "status": "feedback_processed",
                "message": "í”¼ë“œë°±ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "row_index": row_index,
                "original_material": original_material,
                "corrected_material": corrected_material,
                "reason": reason,
                "production_name": production_name,
                "process": process,
                "ai_processed": AI_AVAILABLE and hasattr(app.state, 'ai_model'),
                "timestamp": datetime.now().isoformat()
            }
            
            return result
                
        except Exception as e:
            logger.error(f"í”¼ë“œë°± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise HTTPException(status_code=500, detail=f"í”¼ë“œë°± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    return app

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = create_app()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8083,
        reload=False
    )
