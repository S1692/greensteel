"""
ë¹„ë™ê¸° ë³‘ë ¬ í•™ìŠµ ëª¨ë“ˆ

RTX 4080ì˜ 16GB VRAMì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ 
ì—¬ëŸ¬ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import json
import torch
import multiprocessing as mp
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import threading
import time

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë¸ í´ë˜ìŠ¤ import
sys.path.append(str(Path(__file__).parent.parent.parent))
from app.ananke.model import XMLRoBERTaClassifier
from app.local_study.main import LocalStudyManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AsyncTrainingManager:
    """ë¹„ë™ê¸° ë³‘ë ¬ í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self, max_concurrent_jobs: int = None):
        """
        Args:
            max_concurrent_jobs: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ì‘ì—… ìˆ˜ (Noneì´ë©´ ìë™ ê³„ì‚°)
        """
        self.gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0
        
        # RTX 4080 (16GB) ê¸°ì¤€ ìµœì í™”
        if max_concurrent_jobs is None:
            if self.gpu_memory_gb >= 15:  # RTX 4080
                # ê° í•™ìŠµ í”„ë¡œì„¸ìŠ¤ê°€ ì•½ 4-5GB ì‚¬ìš©í•˜ë¯€ë¡œ 3ê°œ ë³‘ë ¬ ì‹¤í–‰
                self.max_concurrent_jobs = 3
            elif self.gpu_memory_gb >= 10:
                self.max_concurrent_jobs = 2
            else:
                self.max_concurrent_jobs = 1
        else:
            self.max_concurrent_jobs = max_concurrent_jobs
            
        logger.info(f"GPU ë©”ëª¨ë¦¬: {self.gpu_memory_gb:.1f}GB, ìµœëŒ€ ë™ì‹œ ì‘ì—…: {self.max_concurrent_jobs}ê°œ")
        
        self.current_jobs = []
        self.completed_jobs = []
        self.job_lock = threading.Lock()
        
    async def train_single_batch_async(self, job_id: int, config: Dict) -> Dict:
        """ë‹¨ì¼ ë°°ì¹˜ ë¹„ë™ê¸° í•™ìŠµ"""
        try:
            logger.info(f"ğŸš€ Job {job_id} ì‹œì‘: epochs={config['epochs']}, batch_size={config['batch_size']}")
            
            # GPU ë©”ëª¨ë¦¬ í• ë‹¹ì„ ìœ„í•œ ì§€ì—°
            await asyncio.sleep(job_id * 2)  # ê° ì‘ì—…ë§ˆë‹¤ 2ì´ˆ ê°„ê²©ìœ¼ë¡œ ì‹œì‘
            
            # ìƒˆ í”„ë¡œì„¸ìŠ¤ì—ì„œ í•™ìŠµ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            with ProcessPoolExecutor(max_workers=1) as executor:
                result = await loop.run_in_executor(
                    executor, 
                    self._run_training_process, 
                    job_id, 
                    config
                )
            
            logger.info(f"âœ… Job {job_id} ì™„ë£Œ: Loss={result.get('final_loss', 'N/A')}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Job {job_id} ì‹¤íŒ¨: {e}")
            return {"job_id": job_id, "status": "failed", "error": str(e)}
    
    def _run_training_process(self, job_id: int, config: Dict) -> Dict:
        """ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ëŠ” í•™ìŠµ í•¨ìˆ˜"""
        try:
            # CUDA ë””ë°”ì´ìŠ¤ ì„¤ì • (ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ë‹¤ë¥¸ CUDA ì»¨í…ìŠ¤íŠ¸)
            torch.cuda.set_device(0)
            torch.cuda.empty_cache()
            
            # í•™ìŠµ ê´€ë¦¬ì ì´ˆê¸°í™”
            study_manager = LocalStudyManager()
            
            # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
            texts, labels = study_manager.prepare_training_data()
            
            # í•™ìŠµ ì‹œì‘ ì‹œê°„
            start_time = time.time()
            
            # ëª¨ë¸ í•™ìŠµ
            training_result = study_manager.train_model(
                texts, 
                labels, 
                epochs=config['epochs'],
                batch_size=config['batch_size'],
                learning_rate=config.get('learning_rate', 2e-5)
            )
            
            # í•™ìŠµ ì™„ë£Œ ì‹œê°„
            end_time = time.time()
            duration = end_time - start_time
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            return {
                "job_id": job_id,
                "status": "completed",
                "duration": duration,
                "training_result": training_result,
                "config": config,
                "final_loss": "N/A",  # ì‹¤ì œ ë¡œìŠ¤ëŠ” í•™ìŠµ ë¡œê·¸ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"í”„ë¡œì„¸ìŠ¤ {job_id} ì˜¤ë¥˜: {e}")
            return {
                "job_id": job_id,
                "status": "failed", 
                "error": str(e),
                "config": config,
                "timestamp": datetime.now().isoformat()
            }
    
    async def run_parallel_training(self, training_configs: List[Dict]) -> List[Dict]:
        """ë³‘ë ¬ í•™ìŠµ ì‹¤í–‰"""
        logger.info(f"ğŸ¯ {len(training_configs)}ê°œ ì‘ì—…ì„ {self.max_concurrent_jobs}ê°œì”© ë³‘ë ¬ ì‹¤í–‰")
        
        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì‘ì—… ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(self.max_concurrent_jobs)
        
        async def limited_train(job_id: int, config: Dict):
            async with semaphore:
                return await self.train_single_batch_async(job_id, config)
        
        # ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        tasks = [
            limited_train(i, config) 
            for i, config in enumerate(training_configs)
        ]
        
        # ëª¨ë“  ì‘ì—… ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì •ë¦¬
        completed_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"ì‘ì—… ì‹¤íŒ¨: {result}")
                completed_results.append({"status": "failed", "error": str(result)})
            else:
                completed_results.append(result)
        
        return completed_results
    
    def generate_training_configs(self, target_loss: float = 2.0) -> List[Dict]:
        """ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ í•™ìŠµ ì„¤ì • ìƒì„±"""
        configs = []
        
        # RTX 4080 ìµœì í™”ëœ ì„¤ì •ë“¤
        base_configs = [
            # ê³ ì† í•™ìŠµ (ì‘ì€ ì—í¬í¬, í° ë°°ì¹˜)
            {"epochs": 10, "batch_size": 128, "learning_rate": 5e-5, "name": "ë¹ ë¥¸í•™ìŠµ"},
            {"epochs": 15, "batch_size": 96, "learning_rate": 3e-5, "name": "ì¤‘ê°„í•™ìŠµ1"},
            {"epochs": 20, "batch_size": 64, "learning_rate": 2e-5, "name": "í‘œì¤€í•™ìŠµ"},
            
            # ì •ë°€ í•™ìŠµ (ë§ì€ ì—í¬í¬, ì‘ì€ ë°°ì¹˜)
            {"epochs": 30, "batch_size": 32, "learning_rate": 1e-5, "name": "ì •ë°€í•™ìŠµ1"},
            {"epochs": 25, "batch_size": 48, "learning_rate": 1.5e-5, "name": "ì •ë°€í•™ìŠµ2"},
            
            # ì‹¤í—˜ì  ì„¤ì •
            {"epochs": 40, "batch_size": 24, "learning_rate": 8e-6, "name": "ëŠë¦°ì •ë°€í•™ìŠµ"},
        ]
        
        return base_configs
    
    async def continuous_training_until_target(self, target_loss: float = 2.0, max_iterations: int = 10):
        """ëª©í‘œ ë¡œìŠ¤ê¹Œì§€ ê³„ì† í•™ìŠµ"""
        logger.info(f"ğŸ¯ ëª©í‘œ ë¡œìŠ¤ {target_loss}ê¹Œì§€ ìµœëŒ€ {max_iterations}íšŒ ë°˜ë³µ í•™ìŠµ")
        
        iteration = 0
        best_loss = float('inf')
        
        while iteration < max_iterations:
            iteration += 1
            logger.info(f"\nğŸ”„ ë°˜ë³µ {iteration}/{max_iterations} ì‹œì‘")
            
            # í•™ìŠµ ì„¤ì • ìƒì„±
            training_configs = self.generate_training_configs(target_loss)
            
            # ë³‘ë ¬ í•™ìŠµ ì‹¤í–‰
            results = await self.run_parallel_training(training_configs)
            
            # ê²°ê³¼ ë¶„ì„
            successful_results = [r for r in results if r.get("status") == "completed"]
            
            if successful_results:
                logger.info(f"âœ… {len(successful_results)}ê°œ ì‘ì—… ì™„ë£Œ")
                
                # ìµœê³  ì„±ëŠ¥ í™•ì¸ (ì‹¤ì œë¡œëŠ” ë¡œê·¸ íŒŒì¼ì—ì„œ ë¡œìŠ¤ ê°’ì„ ì½ì–´ì™€ì•¼ í•¨)
                for result in successful_results:
                    config_name = result['config'].get('name', 'Unknown')
                    duration = result.get('duration', 0)
                    logger.info(f"  ğŸ“Š {config_name}: {duration:.1f}ì´ˆ ì†Œìš”")
                
                # ì ì‹œ ëŒ€ê¸° (GPU ì¿¨ë‹¤ìš´)
                await asyncio.sleep(10)
            else:
                logger.warning("âš ï¸ ëª¨ë“  ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                break
        
        logger.info(f"ğŸ‰ ì´ {iteration}íšŒ ë°˜ë³µ í•™ìŠµ ì™„ë£Œ")


async def main():
    """ë©”ì¸ ë¹„ë™ê¸° ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ ë¹„ë™ê¸° ë³‘ë ¬ í•™ìŠµ ì‹œì‘")
        
        # GPU í™•ì¸
        if not torch.cuda.is_available():
            logger.error("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"ğŸ–¥ï¸ GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # ë¹„ë™ê¸° í•™ìŠµ ê´€ë¦¬ì ì´ˆê¸°í™”
        training_manager = AsyncTrainingManager()
        
        # ì§€ì†ì  í•™ìŠµ ì‹¤í–‰
        await training_manager.continuous_training_until_target(target_loss=2.0, max_iterations=5)
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise


def run_async_training():
    """ë¹„ë™ê¸° í•™ìŠµ ì‹¤í–‰ wrapper"""
    try:
        # Windowsì—ì„œ multiprocessing ì´ìŠˆ í•´ê²°
        if sys.platform.startswith('win'):
            mp.set_start_method('spawn', force=True)
        
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        asyncio.run(main())
        
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    run_async_training()
