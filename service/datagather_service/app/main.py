from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging
from datetime import datetime
from pathlib import Path
import os

# GPU ì„¤ì • ê°•í™”
import torch
if torch.cuda.is_available():
    # CUDA í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # ì²« ë²ˆì§¸ GPU ì‚¬ìš©
    
    # ì‹¤ì œ GPU ì •ë³´ í™•ì¸
    gpu_count = torch.cuda.device_count()
    current_device = torch.cuda.current_device()
    gpu_name = torch.cuda.get_device_name(current_device)
    gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1024**3
    
    print(f"ğŸš€ GPU ì‚¬ìš© ì„¤ì •:")
    print(f"  ğŸ“± GPU ê°œìˆ˜: {gpu_count}ê°œ")
    print(f"  ğŸ¯ í˜„ì¬ GPU: {current_device}ë²ˆ")
    print(f"  ğŸ”§ GPU ëª¨ë¸: {gpu_name}")
    print(f"  ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
else:
    print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

# ì„œë¸Œë¼ìš°í„° import
from .filtering.main import app as filtering_app

# AI ëª¨ë¸ ì§ì ‘ ë¡œë“œ
from .ananke.model import XMLRoBERTaClassifier

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    logger.info(f"DataGather Service starting up...")
    logger.info("Domain: Data Collection & Processing")
    logger.info("Architecture: Modular Design with Sub-routers")
    
    # AI ëª¨ë¸ ì´ˆê¸°í™”
    try:
        logger.info("ğŸ¤– AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
        model_dir = os.path.join(os.path.dirname(__file__), "data", "studied", "model_v24", "model_v24")
        model_dir = os.path.abspath(model_dir)
        
        logger.info(f"ğŸ” ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_dir}")
        logger.info(f"ğŸ” ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¡´ì¬: {os.path.exists(model_dir)}")
        
        if os.path.exists(model_dir):
            # AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            # AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            logger.info("ğŸ”„ XMLRoBERTaClassifier ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì¤‘...")
            try:
                app.state.ai_model = XMLRoBERTaClassifier(model_dir=model_dir)
                logger.info("âœ… AI ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
                
                # ëª¨ë¸ ìƒíƒœ ìƒì„¸ í™•ì¸
                if hasattr(app.state.ai_model, 'training_data'):
                    logger.info(f"âœ… í•™ìŠµ ë°ì´í„° ë¡œë“œ í™•ì¸: {len(app.state.ai_model.training_data)}ê°œ ë¼ë²¨")
                else:
                    logger.warning("âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    
                if hasattr(app.state.ai_model, 'input_text_embeddings_cache'):
                    logger.info(f"âœ… ì„ë² ë”© ìºì‹œ ë¡œë“œ í™•ì¸: {len(app.state.ai_model.input_text_embeddings_cache)}ê°œ")
                else:
                    logger.warning("âš ï¸ ì„ë² ë”© ìºì‹œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    
                # ìµœì¢… ëª¨ë¸ ìƒíƒœ í™•ì¸
                logger.info(f"ğŸ” ìµœì¢… ëª¨ë¸ ìƒíƒœ: {type(app.state.ai_model)}")
                logger.info(f"ğŸ” app.state.ai_model ì¡´ì¬: {hasattr(app.state, 'ai_model')}")
                logger.info(f"ğŸ” app.state.ai_model ê°’: {app.state.ai_model}")
                
            except Exception as model_error:
                logger.error(f"âŒ XMLRoBERTaClassifier ìƒì„± ì‹¤íŒ¨: {model_error}")
                import traceback
                logger.error(f"ğŸ” ëª¨ë¸ ìƒì„± ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
                app.state.ai_model = None
        else:
            logger.error(f"âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_dir}")
            app.state.ai_model = None
            
    except Exception as e:
        logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        logger.error(f"ğŸ” ì „ì²´ ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
        app.state.ai_model = None
    
    yield
    # ì¢…ë£Œ ì‹œ
    logger.info(f"DataGather Service shutting down...")

# ë©”ì¸ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
app = FastAPI(
    title="DataGather Service - Modular Architecture",
    description="ESG ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì„œë¹„ìŠ¤ - ëª¨ë“ˆí˜• ì„¤ê³„",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:8080"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# ì„œë¸Œë¼ìš°í„° ë§ˆìš´íŠ¸
app.mount("/filtering", filtering_app)

# í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "ok",
        "service": "datagather",
        "domain": "data-collection",
        "architecture": "Modular with Sub-routers",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "modules": ["filtering"],
        "ai_model_status": "loaded" if hasattr(app.state, 'ai_model') and app.state.ai_model else "not_loaded"
    }

# AI ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.post("/ai-test")
async def ai_test(text: str):
    """AI ëª¨ë¸ ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    if not hasattr(app.state, 'ai_model') or app.state.ai_model is None:
        return {"error": "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
    
    try:
        # AI ëª¨ë¸ ìƒíƒœ ìƒì„¸ í™•ì¸
        model = app.state.ai_model
        model_status = {
            "model_type": type(model).__name__,
            "has_training_data": hasattr(model, 'training_data'),
            "training_data_count": len(model.training_data) if hasattr(model, 'training_data') else 0,
            "has_embeddings_cache": hasattr(model, 'input_text_embeddings_cache'),
            "embeddings_cache_count": len(model.input_text_embeddings_cache) if hasattr(model, 'input_text_embeddings_cache') else 0,
            "device": str(model.device) if hasattr(model, 'device') else "unknown"
        }
        
        # ì²  ë¼ë²¨ ìƒì„¸ í™•ì¸
        if hasattr(model, 'training_data') and 'ì² ' in model.training_data:
            ì² _ë°ì´í„° = model.training_data['ì² ']
            ì² _ìƒì„¸ = {
                "ì² _ë¼ë²¨_ì¡´ì¬": True,
                "ì² _input_texts_ìˆ˜": len(ì² _ë°ì´í„°),
                "ì² _input_texts_ìƒ˜í”Œ": ì² _ë°ì´í„°[:5],  # ì²˜ìŒ 5ê°œë§Œ
                "ì² ã„¹_í¬í•¨": 'ì² ã„¹' in ì² _ë°ì´í„°,
                "ì² ã„¹_ìœ„ì¹˜": ì² _ë°ì´í„°.index('ì² ã„¹') + 1 if 'ì² ã„¹' in ì² _ë°ì´í„° else -1
            }
        else:
            ì² _ìƒì„¸ = {"ì² _ë¼ë²¨_ì¡´ì¬": False}
        
        # AI ëª¨ë¸ë¡œ ì˜ˆì¸¡
        result = await model.predict(text)
        
        return {
            "input": text,
            "result": result,
            "model_status": model_status,
            "ì² _ìƒì„¸": ì² _ìƒì„¸,
            "model_loaded": True
        }
    except Exception as e:
        import traceback
        return {
            "error": f"AI ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}",
            "traceback": traceback.format_exc(),
            "model_loaded": hasattr(app.state, 'ai_model') and app.state.ai_model is not None
        }

# ë£¨íŠ¸ ê²½ë¡œ
@app.get("/")
async def root():
    """ë£¨íŠ¸ ê²½ë¡œ"""
    return {
        "service": "DataGather Service",
        "version": "1.0.0",
        "domain": "Data Collection & Processing",
        "architecture": "Modular with Sub-routers",
        "endpoints": {
            "health": "/health",
            "filtering": "/filtering",
            "documentation": "/docs"
        },
        "sub_routers": {
            "filtering": {
                "description": "AI ëª¨ë¸ì„ í™œìš©í•œ íˆ¬ì…ë¬¼ëª… ë¶„ë¥˜ ë° ìˆ˜ì •",
                "endpoints": {
                    "ai_process": "/filtering/ai-process",
                    "feedback": "/filtering/feedback",
                    "process_data": "/filtering/process-data"
                }
            }
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8083,
        reload=False
    )
