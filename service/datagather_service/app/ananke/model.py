import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import numpy as np
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple
import pickle
import pandas as pd

class XMLRoBERTaClassifier:
    def __init__(self, model_name="xlm-roberta-base", model_dir=None):
        # GPU ì„¤ì • ê°•í™”
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            torch.cuda.empty_cache()
            # í˜¼í•© ì •ë°€ë„ í•™ìŠµ í™œì„±í™”
            torch.backends.cudnn.benchmark = True
            print(f"GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        else:
            self.device = torch.device("cpu")
            print("CPU ì‚¬ìš©")
        
        self.model_name = model_name
        
        # ë ˆì´ë¸” ë§¤í•‘ ì´ˆê¸°í™” (ë¨¼ì €!)
        self.label_to_id = {}
        self.id_to_label = {}
        self.label_embeddings = {}
        
        if model_dir and os.path.exists(model_dir):
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
            self.model = AutoModel.from_pretrained(model_dir)
            # ë¶„ë¥˜ê¸° í¬ê¸°ëŠ” load_classifierì—ì„œ ì„¤ì •ë¨
            self.load_classifier(model_dir)
        else:
            # ìƒˆ ëª¨ë¸ ì´ˆê¸°í™” (ë¶„ë¥˜ê¸° í¬ê¸°ëŠ” ë‚˜ì¤‘ì— ì„¤ì •)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            # ë¶„ë¥˜ê¸°ëŠ” prepare_training_dataì—ì„œ ìƒì„±ë¨
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™ ë° ìµœì í™”
        self.model.to(self.device)
        
        # ë¶„ë¥˜ê¸°ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì„ì‹œë¡œ ìƒì„± (prepare_training_dataì—ì„œ ì¬ìƒì„±ë¨)
        if not hasattr(self, 'classifier'):
            self.classifier = None
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"ëª¨ë¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
        
    def load_classifier(self, model_dir):
        """ì €ì¥ëœ ë¶„ë¥˜ê¸° ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        classifier_path = os.path.join(model_dir, "classifier.pkl")
        if os.path.exists(classifier_path):
            with open(classifier_path, 'rb') as f:
                self.classifier = pickle.load(f)
                self.classifier.to(self.device)
                print(f"ë¶„ë¥˜ê¸° ë¡œë“œ ì™„ë£Œ: {self.classifier.in_features} â†’ {self.classifier.out_features}")
        else:
            print(f"ë¶„ë¥˜ê¸° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {classifier_path}")
        
        # ë ˆì´ë¸” ë§¤í•‘ ë¡œë“œ
        label_map_path = os.path.join(model_dir, "label_mapping.json")
        if os.path.exists(label_map_path):
            with open(label_map_path, 'r', encoding='utf-8') as f:
                self.label_to_id = json.load(f)
                self.id_to_label = {v: k for k, v in self.label_to_id.items()}
        
        # ë ˆì´ë¸” ì„ë² ë”© ë¡œë“œ
        embeddings_path = os.path.join(model_dir, "label_embeddings.pkl")
        if os.path.exists(embeddings_path):
            with open(embeddings_path, 'rb') as f:
                self.label_embeddings = pickle.load(f)
                print(f"ë ˆì´ë¸” ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {len(self.label_embeddings)}ê°œ")
        else:
            print(f"ë ˆì´ë¸” ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {embeddings_path}")
    
    def save_model(self, save_dir):
        """ëª¨ë¸ê³¼ ë¶„ë¥˜ê¸°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        os.makedirs(save_dir, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        self.model.save_pretrained(save_dir)
        self.tokenizer.save_pretrained(save_dir)
        
        # ë¶„ë¥˜ê¸° ì €ì¥
        with open(os.path.join(save_dir, "classifier.pkl"), 'wb') as f:
            pickle.dump(self.classifier, f)
        
        # ë ˆì´ë¸” ë§¤í•‘ ì €ì¥
        with open(os.path.join(save_dir, "label_mapping.json"), 'w', encoding='utf-8') as f:
            json.dump(self.label_to_id, f, ensure_ascii=False, indent=2)
        
        # ë ˆì´ë¸” ì„ë² ë”© ì €ì¥
        with open(os.path.join(save_dir, "label_embeddings.pkl"), 'wb') as f:
            pickle.dump(self.label_embeddings, f)
    
    def prepare_training_data(self, jsonl_path):
        """JSONL íŒŒì¼ì—ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""
        texts = []
        labels = []
        
        print(f"JSONL íŒŒì¼ ê²½ë¡œ: {jsonl_path}")
        print(f"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(jsonl_path)}")
        
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data = json.loads(line.strip())
                    print(f"ë¼ì¸ {i+1}: {list(data.keys())}")
                    
                    # ê° input_textë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ ì²˜ë¦¬ (1~10)
                    label = data.get('Label', '')
                    if not label:
                        print(f"ë¼ì¸ {i+1}: ë¼ë²¨ì´ ì—†ìŒ, ìŠ¤í‚µ")
                        continue
                    
                    sample_count = 0
                    for j in range(1, 11):  # input_text 1ë¶€í„° 10ê¹Œì§€
                        text_key = f'input_text {j}'
                        text = data.get(text_key, '').strip()
                        
                        if text:  # í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê°œë³„ ìƒ˜í”Œë¡œ ì¶”ê°€
                            texts.append(text)
                            labels.append(label)
                            sample_count += 1
                            print(f"ìƒ˜í”Œ ì¶”ê°€: {label} -> {text[:30]}...")
                    
                    print(f"ë¼ì¸ {i+1} ({label}): {sample_count}ê°œ ìƒ˜í”Œ ì¶”ê°€")
                except json.JSONDecodeError as e:
                    print(f"JSON íŒŒì‹± ì˜¤ë¥˜ ë¼ì¸ {i+1}: {e}")
        
        print(f"ìµœì¢… ê²°ê³¼: {len(texts)}ê°œ í…ìŠ¤íŠ¸, {len(labels)}ê°œ ë¼ë²¨")
        
        # ë ˆì´ë¸” ë§¤í•‘ ìƒì„±
        unique_labels = list(set(labels))
        self.label_to_id = {label: i for i, label in enumerate(unique_labels)}
        self.id_to_label = {i: label for label, i in self.label_to_id.items()}
        
        print(f"ë ˆì´ë¸” ë§¤í•‘ ìƒì„± ì™„ë£Œ: {len(self.label_to_id)}ê°œ ê³ ìœ  ë¼ë²¨")
        
        # ì˜¬ë°”ë¥¸ í¬ê¸°ì˜ ë¶„ë¥˜ê¸° ìƒì„± (ë ˆì´ë¸” ìˆ˜ì— ë§ì¶¤)
        num_labels = len(self.label_to_id)
        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)
        self.classifier.to(self.device)
        
        print(f"ë¶„ë¥˜ê¸° ìƒì„± ì™„ë£Œ: ì…ë ¥ {self.model.config.hidden_size} â†’ ì¶œë ¥ {num_labels}")
        
        # ë ˆì´ë¸” ì„ë² ë”© ìƒì„±
        self._create_label_embeddings(texts, labels)
        
        return texts, labels
    
    def _create_label_embeddings(self, texts, labels):
        """ë ˆì´ë¸”ë³„ë¡œ ëŒ€í‘œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        label_texts = {}
        for text, label in zip(texts, labels):
            if label not in label_texts:
                label_texts[label] = []
            label_texts[label].append(text)
        
        self.label_embeddings = {}
        for label, texts_list in label_texts.items():
            # í•´ë‹¹ ë ˆì´ë¸”ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ì— ëŒ€í•œ í‰ê·  ì„ë² ë”© ê³„ì‚°
            embeddings = []
            for text in texts_list[:10]:  # ìµœëŒ€ 10ê°œ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
                inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                    embeddings.append(embedding)
            
            if embeddings:
                self.label_embeddings[label] = np.mean(embeddings, axis=0)
    
    def train(self, texts, labels, epochs=3, batch_size=8, learning_rate=2e-5):
        """ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤."""
        
        # RTX 4080 ìµœì í™”ë¥¼ ìœ„í•œ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_memory >= 16:  # RTX 4080 (16GB) ì´ìƒ
                if batch_size < 32:
                    batch_size = min(64, max(32, batch_size))  # ìµœì†Œ 32, ìµœëŒ€ 64
                    learning_rate = min(5e-5, learning_rate * 2)  # í° ë°°ì¹˜ì— ë§ì¶° í•™ìŠµë¥  ì¡°ì •
                    print(f"ğŸš€ RTX 4080 ìµœì í™”: ë°°ì¹˜ í¬ê¸° {batch_size}, í•™ìŠµë¥  {learning_rate}")
        
        print(f"\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
        print(f"{'='*60}")
        print(f"ğŸ“Š í•™ìŠµ ì„¤ì •:")
        print(f"  ğŸ“ ìƒ˜í”Œ ìˆ˜: {len(texts)}ê°œ")
        print(f"  ğŸ·ï¸  ê³ ìœ  ë¼ë²¨: {len(set(labels))}ê°œ")
        print(f"  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"  ğŸ“š í•™ìŠµë¥ : {learning_rate}")
        print(f"  ğŸ”„ ì—í¬í¬: {epochs}")
        print(f"  ğŸ’» ì¥ì¹˜: {self.device}")
        if torch.cuda.is_available():
            print(f"  ğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}")
            print(f"  ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            print(f"  ğŸ”¥ í˜¼í•© ì •ë°€ë„ í•™ìŠµ: í™œì„±í™”")
            print(f"  âš¡ GPU ìµœì í™”: RTX 4080 ì „ìš© ì„¤ì •")
        print(f"{'='*60}\n")
        
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (í•™ìŠµ ì „): {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
        
        # ë°ì´í„° í† í¬ë‚˜ì´ì§•
        encodings = self.tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors="pt")
        
        # ë ˆì´ë¸”ì„ IDë¡œ ë³€í™˜
        label_ids = [self.label_to_id[label] for label in labels]
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = torch.utils.data.TensorDataset(
            encodings['input_ids'],
            encodings['attention_mask'],
            torch.tensor(label_ids)
        )
        
        # ë°ì´í„°ë¡œë” ìƒì„± (GPU ìµœì í™”)
        dataloader = torch.utils.data.DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=True,
            pin_memory=True if torch.cuda.is_available() else False,
            num_workers=0 if torch.cuda.is_available() else 2  # GPU ì‚¬ìš© ì‹œ ë‹¨ì¼ ì›Œì»¤
        )
        
        # RTX 4080 ìµœì í™” ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
        optimizer = torch.optim.AdamW(
            list(self.model.parameters()) + list(self.classifier.parameters()), 
            lr=learning_rate,
            weight_decay=0.01,  # ì •ê·œí™”
            eps=1e-6  # RTX 4080 í˜¼í•© ì •ë°€ë„ ìµœì í™”
        )
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€ (GPU íš¨ìœ¨ì„± í–¥ìƒ)
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=learning_rate/10)
        
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # ë¼ë²¨ ìŠ¤ë¬´ë”©ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
        
        # RTX 4080 í˜¼í•© ì •ë°€ë„ ìµœì í™”ë¥¼ ìœ„í•œ GradScaler
        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None
        
        # í•™ìŠµ ë£¨í”„
        self.model.train()
        self.classifier.train()
        
        for epoch in range(epochs):
            total_loss = 0
            correct_predictions = 0
            total_predictions = 0
            
            print(f"\n{'='*50}")
            print(f"Epoch {epoch+1}/{epochs} ì‹œì‘")
            print(f"{'='*50}")
            
            for batch_idx, batch in enumerate(dataloader):
                input_ids, attention_mask, label_ids = [b.to(self.device, non_blocking=True) for b in batch]
                
                optimizer.zero_grad()
                
                # RTX 4080 ìµœì í™” í˜¼í•© ì •ë°€ë„ í•™ìŠµ
                if torch.cuda.is_available() and scaler is not None:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                        logits = self.classifier(outputs.last_hidden_state[:, 0, :])
                        loss = criterion(logits, label_ids)
                    
                    # GradScalerë¡œ ì—­ì „íŒŒ ìµœì í™”
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    logits = self.classifier(outputs.last_hidden_state[:, 0, :])
                    loss = criterion(logits, label_ids)
                    loss.backward()
                    optimizer.step()
                
                total_loss += loss.item()
                
                # ì •í™•ë„ ê³„ì‚° (í•™ìŠµ ì¤‘)
                _, predicted = torch.max(logits, 1)
                correct_predictions += (predicted == label_ids).sum().item()
                total_predictions += label_ids.size(0)
                
                # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© ì¶œë ¥ (10ë°°ì¹˜ë§ˆë‹¤)
                if batch_idx % 10 == 0:
                    batch_accuracy = (predicted == label_ids).sum().item() / label_ids.size(0)
                    print(f"  Batch {batch_idx+1}/{len(dataloader)}: Loss={loss.item():.4f}, Accuracy={batch_accuracy:.2%}")
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if torch.cuda.is_available() and batch_idx % 10 == 0:
                    torch.cuda.empty_cache()
            
            # ì—í¬í¬ë³„ ì „ì²´ ì§€í‘œ
            avg_loss = total_loss / len(dataloader)
            epoch_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
            
            print(f"\nğŸ“Š Epoch {epoch+1}/{epochs} ê²°ê³¼:")
            print(f"  ğŸ“‰ í‰ê·  Loss: {avg_loss:.6f}")
            print(f"  ğŸ¯ ì •í™•ë„: {epoch_accuracy:.2%} ({correct_predictions}/{total_predictions})")
            print(f"  â±ï¸  ë°°ì¹˜ ìˆ˜: {len(dataloader)}")
            
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated(0) / 1024**3
                print(f"  ğŸ–¥ï¸  GPU ë©”ëª¨ë¦¬: {gpu_memory:.2f}GB")
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            current_lr = scheduler.get_last_lr()[0]
            scheduler.step()
            next_lr = scheduler.get_last_lr()[0]
            print(f"  ğŸ“š í•™ìŠµë¥ : {current_lr:.2e} â†’ {next_lr:.2e}")
            
            print(f"{'='*50}")
        
        # í•™ìŠµ ì™„ë£Œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
            print(f"{'='*60}")
            print(f"ğŸ“Š ìµœì¢… í•™ìŠµ ê²°ê³¼:")
            print(f"  ğŸ”„ ì´ ì—í¬í¬: {epochs}")
            print(f"  ğŸ“ ì´ ìƒ˜í”Œ: {len(texts)}ê°œ")
            print(f"  ğŸ·ï¸  ê³ ìœ  ë¼ë²¨: {len(set(labels))}ê°œ")
            print(f"  ğŸ–¥ï¸  GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
            print(f"{'='*60}\n")
    
    def predict(self, text, top_k=3):
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        print(f"ì˜ˆì¸¡ ì‹œì‘: '{text}', ë ˆì´ë¸” ì„ë² ë”© ìˆ˜: {len(self.label_embeddings)}")
        self.model.eval()
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            text_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
        
        print(f"í…ìŠ¤íŠ¸ ì„ë² ë”© í¬ê¸°: {text_embedding.shape}")
        
        # ë ˆì´ë¸” ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = {}
        for label, label_embedding in self.label_embeddings.items():
            try:
                # ì°¨ì› ë§ì¶”ê¸°: label_embeddingì´ (1, 768)ì´ë©´ flatten
                if label_embedding.ndim > 1:
                    label_embedding = label_embedding.flatten()
                similarity = self._cosine_similarity(text_embedding[0], label_embedding)
                similarities[label] = similarity
            except Exception as e:
                print(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜ ({label}): {e}")
                print(f"label_embedding í¬ê¸°: {label_embedding.shape if hasattr(label_embedding, 'shape') else type(label_embedding)}")
        
        print(f"ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: {len(similarities)}ê°œ")
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
        results = []
        for i, (label, similarity) in enumerate(sorted_similarities[:top_k]):
            results.append({
                'rank': i + 1,
                'label': label,
                'similarity': float(similarity * 100)  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
            })
        
        print(f"ìµœì¢… ê²°ê³¼ ìˆ˜: {len(results)}")
        return results
    
    def _cosine_similarity(self, vec1, vec2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        return dot_product / (norm1 * norm2)
    
    def update_with_feedback(self, text, correct_label, is_correct, memo=""):
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        if is_correct:
            # ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ì´ì—ˆì„ ê²½ìš°, í•´ë‹¹ ë ˆì´ë¸”ì˜ ì„ë² ë”©ì„ ê°•í™”
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                text_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            # ê¸°ì¡´ ì„ë² ë”©ê³¼ ìƒˆë¡œìš´ ì„ë² ë”©ì„ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            if correct_label in self.label_embeddings:
                old_embedding = self.label_embeddings[correct_label]
                new_embedding = text_embedding[0]
                # ê°€ì¤‘ í‰ê·  (ê¸°ì¡´: 0.7, ìƒˆë¡œìš´: 0.3)
                self.label_embeddings[correct_label] = 0.7 * old_embedding + 0.3 * new_embedding
            else:
                self.label_embeddings[correct_label] = text_embedding[0]
        else:
            # í‹€ë¦° ì˜ˆì¸¡ì´ì—ˆì„ ê²½ìš°, ìƒˆë¡œìš´ ë ˆì´ë¸” ì¶”ê°€ ë˜ëŠ” ê¸°ì¡´ ë ˆì´ë¸” ì—…ë°ì´íŠ¸
            if correct_label not in self.label_to_id:
                # ìƒˆë¡œìš´ ë ˆì´ë¸” ì¶”ê°€
                new_id = len(self.label_to_id)
                self.label_to_id[correct_label] = new_id
                self.id_to_label[new_id] = correct_label
            
            # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¡œ í•´ë‹¹ ë ˆì´ë¸”ì˜ ì„ë² ë”© ìƒì„±
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                text_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            self.label_embeddings[correct_label] = text_embedding[0]
        
        # ë©”ëª¨ê°€ ìˆë‹¤ë©´ ì €ì¥ (í–¥í›„ ë”¥ëŸ¬ë‹ì„ ìœ„í•´)
        if memo:
            memo_file = Path(__file__).parent.parent.parent.parent / "data" / "studied" / "feedback_memos.jsonl"
            memo_file.parent.mkdir(exist_ok=True)
            
            memo_data = {
                'text': text,
                'correct_label': correct_label,
                'memo': memo,
                'timestamp': str(pd.Timestamp.now())
            }
            
            with open(memo_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(memo_data, ensure_ascii=False) + '\n')
