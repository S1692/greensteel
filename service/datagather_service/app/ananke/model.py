import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import numpy as np
import json
import os
from pathlib import Path
from typing import List, Dict, Tuple
import pickle
import pandas as pd

class XMLRoBERTaClassifier:
    def __init__(self, model_name="xlm-roberta-base", model_dir=None):
        # GPU ì„¤ì • ê°•í™” - ìš°ì„ ì ìœ¼ë¡œ GPU ì‚¬ìš©
        if torch.cuda.is_available():
            # CUDA í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
            import os
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
            
            self.device = torch.device("cuda")
            # GPU ë©”ëª¨ë¦¬ ìµœì í™”
            torch.cuda.empty_cache()
            # í˜¼í•© ì •ë°€ë„ í•™ìŠµ í™œì„±í™”
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False  # ì„±ëŠ¥ ìµœì í™”
            
            try:
                print(f"ğŸš€ GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
                print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
                # GPU ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹
                torch.cuda.set_per_process_memory_fraction(0.8)
            except Exception as e:
                print(f"âš ï¸ GPU ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                print("GPU ì‚¬ìš©ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ì •ë³´ ì¡°íšŒì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            print("CUDA ì„¤ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        self.model_name = model_name
        
        # ë ˆì´ë¸” ë§¤í•‘ ì´ˆê¸°í™” (ë¨¼ì €!)
        self.label_to_id = {}
        self.id_to_label = {}
        self.label_embeddings = {}
        
        # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹œ ì´ˆê¸°í™”
        self.input_text_embeddings_cache = {}  # input_text ì„ë² ë”© ìºì‹œ
        self.batch_size = 32  # ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        
        if model_dir and os.path.exists(model_dir):
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
            self.model = AutoModel.from_pretrained(model_dir)
            # ë¶„ë¥˜ê¸° í¬ê¸°ëŠ” load_classifierì—ì„œ ì„¤ì •ë¨
            self.load_classifier(model_dir)
        else:
            # ìƒˆ ëª¨ë¸ ì´ˆê¸°í™” (ë¶„ë¥˜ê¸° í¬ê¸°ëŠ” ë‚˜ì¤‘ì— ì„¤ì •)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            # ê¸°ë³¸ ë¶„ë¥˜ê¸° ìƒì„± (ì„ì‹œ)
            self.classifier = nn.Linear(self.model.config.hidden_size, 1)
            self.classifier.to(self.device)
            print(f"ê¸°ë³¸ ë¶„ë¥˜ê¸° ìƒì„±: ì…ë ¥ {self.model.config.hidden_size} â†’ ì¶œë ¥ 1")
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™ ë° ìµœì í™”
        self.model.to(self.device)
        
        # ë¶„ë¥˜ê¸°ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì„ì‹œë¡œ ìƒì„± (prepare_training_dataì—ì„œ ì¬ìƒì„±ë¨)
        if not hasattr(self, 'classifier') or self.classifier is None:
            self.classifier = nn.Linear(self.model.config.hidden_size, 1)
            self.classifier.to(self.device)
            print(f"ì„ì‹œ ë¶„ë¥˜ê¸° ìƒì„±: ì…ë ¥ {self.model.config.hidden_size} â†’ ì¶œë ¥ 1")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"ëª¨ë¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
        
    def load_classifier(self, model_dir):
        """ì €ì¥ëœ ë¶„ë¥˜ê¸° ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        classifier_path = os.path.join(model_dir, "classifier.pkl")
        if os.path.exists(classifier_path):
            with open(classifier_path, 'rb') as f:
                # GPU/CPU í˜¸í™˜ì„±ì„ ìœ„í•œ map_location ì„¤ì •
                if torch.cuda.is_available():
                    self.classifier = pickle.load(f)
                else:
                    # CPUì—ì„œ GPU ëª¨ë¸ì„ ë¡œë“œí•  ë•Œ map_location ì‚¬ìš©
                    import io
                    buffer = f.read()
                    self.classifier = torch.load(io.BytesIO(buffer), map_location=torch.device('cpu'))
                
                self.classifier.to(self.device)
                print(f"âœ… ë¶„ë¥˜ê¸° ë¡œë“œ ì™„ë£Œ: {self.classifier.in_features} â†’ {self.classifier.out_features}")
                print(f"ğŸ“± ì¥ì¹˜: {next(self.classifier.parameters()).device}")
        else:
            print(f"âŒ ë¶„ë¥˜ê¸° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {classifier_path}")
        
        # í•™ìŠµëœ ë°ì´í„° ì´ˆê¸°í™”
        self.training_data = {}
        
        # ë ˆì´ë¸” ë§¤í•‘ ë¡œë“œ
        label_map_path = os.path.join(model_dir, "label_mapping.json")
        if os.path.exists(label_map_path):
            with open(label_map_path, 'r', encoding='utf-8') as f:
                self.label_to_id = json.load(f)
            
            # í•™ìŠµëœ ë°ì´í„° ë¡œë“œ
            self.load_training_data(model_dir)
                self.id_to_label = {v: k for k, v in self.label_to_id.items()}
        
        # ë ˆì´ë¸” ì„ë² ë”© ë¡œë“œ
        embeddings_path = os.path.join(model_dir, "label_embeddings.pkl")
        if os.path.exists(embeddings_path):
            with open(embeddings_path, 'rb') as f:
                self.label_embeddings = pickle.load(f)
                print(f"ë ˆì´ë¸” ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {len(self.label_embeddings)}ê°œ")
        else:
            print(f"ë ˆì´ë¸” ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {embeddings_path}")
    
    def save_model(self, save_dir):
        """ëª¨ë¸ê³¼ ë¶„ë¥˜ê¸°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        os.makedirs(save_dir, exist_ok=True)
        
        # ëª¨ë¸ ì €ì¥
        self.model.save_pretrained(save_dir)
        self.tokenizer.save_pretrained(save_dir)
        
        # ë¶„ë¥˜ê¸° ì €ì¥
        with open(os.path.join(save_dir, "classifier.pkl"), 'wb') as f:
            pickle.dump(self.classifier, f)
        
        # ë ˆì´ë¸” ë§¤í•‘ ì €ì¥
        with open(os.path.join(save_dir, "label_mapping.json"), 'w', encoding='utf-8') as f:
            json.dump(self.label_to_id, f, ensure_ascii=False, indent=2)
        
        # ë ˆì´ë¸” ì„ë² ë”© ì €ì¥
        with open(os.path.join(save_dir, "label_embeddings.pkl"), 'wb') as f:
            pickle.dump(self.label_embeddings, f)
    
    def prepare_training_data(self, jsonl_path):
        """JSONL íŒŒì¼ì—ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."""
        texts = []
        labels = []
        
        print(f"JSONL íŒŒì¼ ê²½ë¡œ: {jsonl_path}")
        print(f"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(jsonl_path)}")
        
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data = json.loads(line.strip())
                    print(f"ë¼ì¸ {i+1}: {list(data.keys())}")
                    
                    # ê° input_textë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ ì²˜ë¦¬ (1~10)
                    label = data.get('Label', '')
                    if not label:
                        print(f"ë¼ì¸ {i+1}: ë¼ë²¨ì´ ì—†ìŒ, ìŠ¤í‚µ")
                        continue
                    
                    sample_count = 0
                    for j in range(1, 11):  # input_text 1ë¶€í„° 10ê¹Œì§€
                        text_key = f'input_text {j}'
                        text = data.get(text_key, '').strip()
                        
                        if text:  # í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê°œë³„ ìƒ˜í”Œë¡œ ì¶”ê°€
                            texts.append(text)
                            labels.append(label)
                            sample_count += 1
                            print(f"ìƒ˜í”Œ ì¶”ê°€: {label} -> {text[:30]}...")
                    
                    print(f"ë¼ì¸ {i+1} ({label}): {sample_count}ê°œ ìƒ˜í”Œ ì¶”ê°€")
                except json.JSONDecodeError as e:
                    print(f"JSON íŒŒì‹± ì˜¤ë¥˜ ë¼ì¸ {i+1}: {e}")
        
        print(f"ìµœì¢… ê²°ê³¼: {len(texts)}ê°œ í…ìŠ¤íŠ¸, {len(labels)}ê°œ ë¼ë²¨")
        
        # ë ˆì´ë¸” ë§¤í•‘ ìƒì„±
        unique_labels = list(set(labels))
        self.label_to_id = {label: i for i, label in enumerate(unique_labels)}
        self.id_to_label = {i: label for label, i in self.label_to_id.items()}
        
        print(f"ë ˆì´ë¸” ë§¤í•‘ ìƒì„± ì™„ë£Œ: {len(self.label_to_id)}ê°œ ê³ ìœ  ë¼ë²¨")
        
        # ì˜¬ë°”ë¥¸ í¬ê¸°ì˜ ë¶„ë¥˜ê¸° ìƒì„± (ë ˆì´ë¸” ìˆ˜ì— ë§ì¶¤)
        num_labels = len(self.label_to_id)
        self.classifier = nn.Linear(self.model.config.hidden_size, num_labels)
        self.classifier.to(self.device)
        
        print(f"ë¶„ë¥˜ê¸° ìƒì„± ì™„ë£Œ: ì…ë ¥ {self.model.config.hidden_size} â†’ ì¶œë ¥ {num_labels}")
        
        # ë ˆì´ë¸” ì„ë² ë”© ìƒì„±
        self._create_label_embeddings(texts, labels)
        
        return texts, labels
    
    def _create_label_embeddings(self, texts, labels):
        """ë ˆì´ë¸”ë³„ë¡œ ëŒ€í‘œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        label_texts = {}
        for text, label in zip(texts, labels):
            if label not in label_texts:
                label_texts[label] = []
            label_texts[label].append(text)
        
        self.label_embeddings = {}
        for label, texts_list in label_texts.items():
            # í•´ë‹¹ ë ˆì´ë¸”ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ì— ëŒ€í•œ í‰ê·  ì„ë² ë”© ê³„ì‚°
            embeddings = []
            for text in texts_list[:10]:  # ìµœëŒ€ 10ê°œ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
                inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                    embeddings.append(embedding)
            
            if embeddings:
                self.label_embeddings[label] = np.mean(embeddings, axis=0)
    
    def train(self, texts, labels, epochs=3, batch_size=8, learning_rate=2e-5):
        """ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤."""
        
        # RTX 4080 ìµœì í™”ë¥¼ ìœ„í•œ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if gpu_memory >= 16:  # RTX 4080 (16GB) ì´ìƒ
                if batch_size < 32:
                    batch_size = min(64, max(32, batch_size))  # ìµœì†Œ 32, ìµœëŒ€ 64
                    learning_rate = min(5e-5, learning_rate * 2)  # í° ë°°ì¹˜ì— ë§ì¶° í•™ìŠµë¥  ì¡°ì •
                    print(f"ğŸš€ RTX 4080 ìµœì í™”: ë°°ì¹˜ í¬ê¸° {batch_size}, í•™ìŠµë¥  {learning_rate}")
        
        print(f"\nğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
        print(f"{'='*60}")
        print(f"ğŸ“Š í•™ìŠµ ì„¤ì •:")
        print(f"  ğŸ“ ìƒ˜í”Œ ìˆ˜: {len(texts)}ê°œ")
        print(f"  ğŸ·ï¸  ê³ ìœ  ë¼ë²¨: {len(set(labels))}ê°œ")
        print(f"  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"  ğŸ“š í•™ìŠµë¥ : {learning_rate}")
        print(f"  ğŸ”„ ì—í¬í¬: {epochs}")
        print(f"  ğŸ’» ì¥ì¹˜: {self.device}")
        if torch.cuda.is_available():
            print(f"  ğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}")
            print(f"  ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            print(f"  ğŸ”¥ í˜¼í•© ì •ë°€ë„ í•™ìŠµ: í™œì„±í™”")
            print(f"  âš¡ GPU ìµœì í™”: RTX 4080 ì „ìš© ì„¤ì •")
        print(f"{'='*60}\n")
        
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (í•™ìŠµ ì „): {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
        
        # ë°ì´í„° í† í¬ë‚˜ì´ì§•
        encodings = self.tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors="pt")
        
        # ë ˆì´ë¸”ì„ IDë¡œ ë³€í™˜
        label_ids = [self.label_to_id[label] for label in labels]
        
        # ë°ì´í„°ì…‹ ìƒì„±
        dataset = torch.utils.data.TensorDataset(
            encodings['input_ids'],
            encodings['attention_mask'],
            torch.tensor(label_ids)
        )
        
        # ë°ì´í„°ë¡œë” ìƒì„± (GPU ìµœì í™”)
        dataloader = torch.utils.data.DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=True,
            pin_memory=True if torch.cuda.is_available() else False,
            num_workers=0 if torch.cuda.is_available() else 2  # GPU ì‚¬ìš© ì‹œ ë‹¨ì¼ ì›Œì»¤
        )
        
        # RTX 4080 ìµœì í™” ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
        optimizer = torch.optim.AdamW(
            list(self.model.parameters()) + list(self.classifier.parameters()), 
            lr=learning_rate,
            weight_decay=0.01,  # ì •ê·œí™”
            eps=1e-6  # RTX 4080 í˜¼í•© ì •ë°€ë„ ìµœì í™”
        )
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€ (GPU íš¨ìœ¨ì„± í–¥ìƒ)
        from torch.optim.lr_scheduler import CosineAnnealingLR
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=learning_rate/10)
        
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # ë¼ë²¨ ìŠ¤ë¬´ë”©ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
        
        # RTX 4080 í˜¼í•© ì •ë°€ë„ ìµœì í™”ë¥¼ ìœ„í•œ GradScaler
        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None
        
        # í•™ìŠµ ë£¨í”„
        self.model.train()
        self.classifier.train()
        
        for epoch in range(epochs):
            total_loss = 0
            correct_predictions = 0
            total_predictions = 0
            
            print(f"\n{'='*50}")
            print(f"Epoch {epoch+1}/{epochs} ì‹œì‘")
            print(f"{'='*50}")
            
            for batch_idx, batch in enumerate(dataloader):
                input_ids, attention_mask, label_ids = [b.to(self.device, non_blocking=True) for b in batch]
                
                optimizer.zero_grad()
                
                # RTX 4080 ìµœì í™” í˜¼í•© ì •ë°€ë„ í•™ìŠµ
                if torch.cuda.is_available() and scaler is not None:
                    with torch.cuda.amp.autocast():
                        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                        logits = self.classifier(outputs.last_hidden_state[:, 0, :])
                        loss = criterion(logits, label_ids)
                    
                    # GradScalerë¡œ ì—­ì „íŒŒ ìµœì í™”
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    logits = self.classifier(outputs.last_hidden_state[:, 0, :])
                    loss = criterion(logits, label_ids)
                    loss.backward()
                    optimizer.step()
                
                total_loss += loss.item()
                
                # ì •í™•ë„ ê³„ì‚° (í•™ìŠµ ì¤‘)
                _, predicted = torch.max(logits, 1)
                correct_predictions += (predicted == label_ids).sum().item()
                total_predictions += label_ids.size(0)
                
                # ë°°ì¹˜ë³„ ì§„í–‰ìƒí™© ì¶œë ¥ (10ë°°ì¹˜ë§ˆë‹¤)
                if batch_idx % 10 == 0:
                    batch_accuracy = (predicted == label_ids).sum().item() / label_ids.size(0)
                    print(f"  Batch {batch_idx+1}/{len(dataloader)}: Loss={loss.item():.4f}, Accuracy={batch_accuracy:.2%}")
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if torch.cuda.is_available() and batch_idx % 10 == 0:
                    torch.cuda.empty_cache()
            
            # ì—í¬í¬ë³„ ì „ì²´ ì§€í‘œ
            avg_loss = total_loss / len(dataloader)
            epoch_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
            
            print(f"\nğŸ“Š Epoch {epoch+1}/{epochs} ê²°ê³¼:")
            print(f"  ğŸ“‰ í‰ê·  Loss: {avg_loss:.6f}")
            print(f"  ğŸ¯ ì •í™•ë„: {epoch_accuracy:.2%} ({correct_predictions}/{total_predictions})")
            print(f"  â±ï¸  ë°°ì¹˜ ìˆ˜: {len(dataloader)}")
            
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated(0) / 1024**3
                print(f"  ğŸ–¥ï¸  GPU ë©”ëª¨ë¦¬: {gpu_memory:.2f}GB")
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            current_lr = scheduler.get_last_lr()[0]
            scheduler.step()
            next_lr = scheduler.get_last_lr()[0]
            print(f"  ğŸ“š í•™ìŠµë¥ : {current_lr:.2e} â†’ {next_lr:.2e}")
            
            print(f"{'='*50}")
        
        # í•™ìŠµ ì™„ë£Œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
            print(f"{'='*60}")
            print(f"ğŸ“Š ìµœì¢… í•™ìŠµ ê²°ê³¼:")
            print(f"  ğŸ”„ ì´ ì—í¬í¬: {epochs}")
            print(f"  ğŸ“ ì´ ìƒ˜í”Œ: {len(texts)}ê°œ")
            print(f"  ğŸ·ï¸  ê³ ìœ  ë¼ë²¨: {len(set(labels))}ê°œ")
            print(f"  ğŸ–¥ï¸  GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB")
            print(f"{'='*60}\n")
    
    async def predict(self, text, top_k=3):
        """
        ê°œì„ ëœ AI ëª¨ë¸ ì˜ˆì¸¡: 100% ì¼ì¹˜, ìœ ì‚¬ë„ ê¸°ë°˜, ë¹„ë™ê¸° ì²˜ë¦¬
        """
        print(f"ğŸ¤– AI ëª¨ë¸ ì˜ˆì¸¡ ì‹œì‘: '{text}'")
        
        # ê¸°ë³¸ ë¶„ë¥˜ê¸°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not hasattr(self, 'classifier') or self.classifier is None:
            print("âŒ ë¶„ë¥˜ê¸°ê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ")
            return []
        
        # í•™ìŠµëœ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not hasattr(self, 'training_data') or not self.training_data:
            print("âš ï¸ í•™ìŠµëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return [{
                'rank': 1,
                'label': text,  # ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜
                'similarity': 100.0,
                'best_match': text,
                'match_type': 'no_training_data'
            }]
        
        print(f"ğŸ“Š í•™ìŠµëœ ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡ ì‹œì‘")
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()
        
        # 1ë‹¨ê³„: 100% ì¼ì¹˜ í™•ì¸ (ê°€ì¥ ë¹ ë¦„)
        exact_match_result = await self._check_exact_match(text)
        if exact_match_result:
            print(f"ğŸ¯ 100% ì¼ì¹˜ ë°œê²¬: {exact_match_result}")
            return [exact_match_result]
        
        # 2ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ì˜ˆì¸¡ (ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬)
        similarity_result = await self._predict_by_similarity(text, top_k)
        if similarity_result:
            return similarity_result
        
        # 3ë‹¨ê³„: ì „í˜€ ìœ ì‚¬í•˜ì§€ ì•ŠìŒ â†’ ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜
        print(f"âš ï¸ ìœ ì‚¬í•œ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return [{
            'rank': 1,
            'label': text,  # ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜
            'similarity': 0.0,
            'best_match': text,
            'match_type': 'no_similarity'
        }]
    
    async def _check_exact_match(self, text):
        """100% ì¼ì¹˜í•˜ëŠ” ë¼ë²¨ì´ë‚˜ í•™ìŠµê°’ì´ ìˆëŠ”ì§€ í™•ì¸"""
        print(f"ğŸ” 100% ì¼ì¹˜ í™•ì¸ ì¤‘: '{text}'")
        
        # 1. ë¼ë²¨ê³¼ ì§ì ‘ ë¹„êµ
        if text in self.training_data:
            print(f"âœ… ë¼ë²¨ê³¼ 100% ì¼ì¹˜: '{text}'")
            return {
                'rank': 1,
                'label': text,
                'similarity': 100.0,
                'best_match': text,
                'match_type': 'exact_label'
            }
        
        # 2. í•™ìŠµê°’ê³¼ ì§ì ‘ ë¹„êµ (ê°•í™”ëœ ë§¤ì¹­)
        for label, input_texts in self.training_data.items():
            # ë””ë²„ê¹…: ì²  ë¼ë²¨ ìƒì„¸ í™•ì¸
            if label == 'ì² ' and text == 'ì² ã„¹':
                print(f"ğŸ” ì²  ë¼ë²¨ ë””ë²„ê¹…:")
                print(f"  ğŸ“ ê²€ìƒ‰ ëŒ€ìƒ: '{text}'")
                print(f"  ğŸ“ ì²  ë¼ë²¨ì˜ input_texts: {input_texts}")
                print(f"  ğŸ“ 'ì² ã„¹' in input_texts: {'ì² ã„¹' in input_texts}")
                
                # ê³µë°± ì •ë³´ ìƒì„¸ ë¶„ì„
                for i, input_text in enumerate(input_texts):
                    if input_text and 'ì² ' in input_text:
                        print(f"  ğŸ” ì²  ê´€ë ¨ input_text {i+1}: '{input_text}' (ê¸¸ì´: {len(input_text)})")
                        print(f"    ğŸ“ ê³µë°± ì œê±° ì „: '{input_text}'")
                        print(f"    ğŸ“ ê³µë°± ì œê±° í›„: '{input_text.strip()}'")
                        print(f"    ğŸ“ ëª¨ë“  ê³µë°± ì œê±°: '{''.join(input_text.split())}'")
                        print(f"    ğŸ“ ê²€ìƒ‰ ëŒ€ìƒ ê³µë°± ì œê±°: '{''.join(text.split())}'")
                        print(f"    ğŸ“ ë§¤ì¹­ ì—¬ë¶€: {''.join(text.split()) == ''.join(input_text.split())}")
            
            # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
            if text in input_texts:
                print(f"âœ… í•™ìŠµê°’ê³¼ 100% ì¼ì¹˜: '{text}' â†’ ë¼ë²¨ '{label}'")
                return {
                    'rank': 1,
                    'label': label,
                    'similarity': 100.0,
                    'best_match': text,
                    'match_type': 'exact_training_data'
                }
            
            # ê³µë°± ì œê±° í›„ ë§¤ì¹­ ì‹œë„
            clean_text = text.strip()
            for input_text in input_texts:
                if input_text and clean_text == input_text.strip():
                    print(f"âœ… í•™ìŠµê°’ê³¼ 100% ì¼ì¹˜ (ê³µë°± ì œê±°): '{text}' â†’ ë¼ë²¨ '{label}'")
                    return {
                        'rank': 1,
                        'label': label,
                        'similarity': 100.0,
                        'best_match': input_text,
                        'match_type': 'exact_training_data'
                    }
            
            # ê³µë°± ë¬´ì‹œ ë§¤ì¹­ ì‹œë„ (ë” ìœ ì—°í•œ ë§¤ì¹­)
            for input_text in input_texts:
                if input_text:
                    # ëª¨ë“  ê³µë°± ì œê±° í›„ ë¹„êµ
                    clean_search_text = ''.join(text.split())
                    clean_input_text = ''.join(input_text.split())
                    
                    if clean_search_text == clean_input_text:
                        print(f"âœ… í•™ìŠµê°’ê³¼ 100% ì¼ì¹˜ (ê³µë°± ë¬´ì‹œ): '{text}' â†’ ë¼ë²¨ '{label}' (ì›ë³¸: '{input_text}')")
                        return {
                            'rank': 1,
                            'label': label,
                            'similarity': 100.0,
                            'best_match': input_text,
                            'match_type': 'exact_training_data_no_spaces'
                        }
            
            # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ì² ã„¹ì´ ì² ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°)
            if text.startswith('ì² '):
                for input_text in input_texts:
                    if input_text and input_text.startswith('ì² '):
                        # ì² ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  input_textì™€ ë¹„êµ
                        clean_search_text = ''.join(text.split())
                        clean_input_text = ''.join(input_text.split())
                        
                        if clean_search_text == clean_input_text:
                            print(f"âœ… ì²  ì‹œì‘ ë¶€ë¶„ ë§¤ì¹­: '{text}' â†’ ë¼ë²¨ '{label}' (ì›ë³¸: '{input_text}')")
                            return {
                                'rank': 1,
                                'label': label,
                                'similarity': 95.0,  # ë¶€ë¶„ ë§¤ì¹­ì´ë¯€ë¡œ 95%
                                'best_match': input_text,
                                'match_type': 'partial_iron_match'
                            }
        
        print(f"âŒ 100% ì¼ì¹˜ ì—†ìŒ")
        return None
    
    async def _predict_by_similarity(self, text, top_k=3):
        """ìœ ì‚¬ë„ ê¸°ë°˜ ì˜ˆì¸¡ (ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬)"""
        print(f"ğŸ” ìœ ì‚¬ë„ ê¸°ë°˜ ì˜ˆì¸¡ ì‹œì‘: '{text}'")
        
        # ì…ë ¥ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        text_embedding = await self._get_text_embedding(text)
        if text_embedding is None:
            return None
        
        # ì„ê³„ê°’ ì„¤ì •
        SIMILARITY_THRESHOLD = 0.7  # 70% ì´ìƒ ìœ ì‚¬í•´ì•¼ í•¨
        
        # ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = await self._calculate_similarities_batch(text_embedding, text)
        
        # ì„ê³„ê°’ ì´ìƒì˜ ê²°ê³¼ë§Œ í•„í„°ë§
        filtered_similarities = {
            label: data for label, data in similarities.items() 
            if data['similarity'] >= SIMILARITY_THRESHOLD
        }
        
        if not filtered_similarities:
            print(f"âš ï¸ ì„ê³„ê°’({SIMILARITY_THRESHOLD}) ì´ìƒì˜ ìœ ì‚¬í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ (ë†’ì€ ìˆœì„œ)
        sorted_similarities = sorted(
            filtered_similarities.items(), 
            key=lambda x: x[1]['similarity'], 
            reverse=True
        )
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
        results = []
        for i, (label, data) in enumerate(sorted_similarities[:top_k]):
            confidence = float(data['similarity'] * 100)
            results.append({
                'rank': i + 1,
                'label': label,
                'similarity': confidence,
                'best_match': data['best_match'],
                'match_type': 'similarity_based'
            })
            print(f"ğŸ† ìˆœìœ„ {i+1}: {label} (ìœ ì‚¬ë„: {confidence:.1f}%, ìµœì  ë§¤ì¹­: {data['best_match']})")
        
        print(f"ğŸ¯ ìœ ì‚¬ë„ ê¸°ë°˜ ì˜ˆì¸¡ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        return results
    
    async def _get_text_embedding(self, text):
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (GPU ìµœì í™”)"""
        try:
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512, 
            padding=True
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            if torch.cuda.is_available():
                with torch.cuda.amp.autocast():
                    outputs = self.model(**inputs)
            else:
                outputs = self.model(**inputs)
            
                # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ
            text_embedding = outputs.last_hidden_state[:, 0, :]
            if torch.cuda.is_available():
                text_embedding = text_embedding.cpu()
            text_embedding = text_embedding.numpy()
        
            return text_embedding
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    async def _calculate_similarities_batch(self, text_embedding, original_text):
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìœ ì‚¬ë„ ê³„ì‚° (ì„±ëŠ¥ í–¥ìƒ)"""
        print(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
        
        similarities = {}
        batch_size = 16  # ë°°ì¹˜ í¬ê¸°
        
        # ëª¨ë“  input_textë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        all_input_texts = []
        for label, input_texts in self.training_data.items():
            for input_text in input_texts:
                if input_text and input_text.strip():
                    all_input_texts.append((label, input_text))
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
        for i in range(0, len(all_input_texts), batch_size):
            batch = all_input_texts[i:i + batch_size]
            
            # ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            batch_texts = [item[1] for item in batch]
            batch_embeddings = await self._get_batch_embeddings(batch_texts)
            
            if batch_embeddings is None:
                continue
            
            # ê° ë°°ì¹˜ í•­ëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            for j, (label, input_text) in enumerate(batch):
                if j < len(batch_embeddings):
                    similarity = self._cosine_similarity(
                        text_embedding[0], 
                        batch_embeddings[j]
                    )
                    
                    # í•´ë‹¹ ë¼ë²¨ì˜ ìµœê³  ìœ ì‚¬ë„ ì—…ë°ì´íŠ¸
                    if label not in similarities or similarity > similarities[label]['similarity']:
                        similarities[label] = {
                            'similarity': similarity,
                            'best_match': input_text
                        }
        
        print(f"âœ… ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: {len(similarities)}ê°œ ë¼ë²¨ ì²˜ë¦¬")
        return similarities
    
    async def _get_batch_embeddings(self, texts):
        """ë°°ì¹˜ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±"""
        try:
            inputs = self.tokenizer(
                texts,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                if torch.cuda.is_available():
                    with torch.cuda.amp.autocast():
                        outputs = self.model(**inputs)
                else:
                    outputs = self.model(**inputs)
                
                # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ
                batch_embeddings = outputs.last_hidden_state[:, 0, :]
                if torch.cuda.is_available():
                    batch_embeddings = batch_embeddings.cpu()
                batch_embeddings = batch_embeddings.numpy()
            
            return batch_embeddings
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def predict_sync(self, text, top_k=3):
        """ë™ê¸° ë²„ì „ì˜ predict (í•˜ìœ„ í˜¸í™˜ì„±)"""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(self.predict(text, top_k))
                loop.close()
                return result
            else:
                return loop.run_until_complete(self.predict(text, top_k))
        except Exception as e:
            print(f"âŒ ë™ê¸° ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return []
    
    def _predict_with_basic_classifier(self, text, top_k=3):
        """ê¸°ë³¸ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©í•œ ì˜ˆì¸¡ (í•™ìŠµëœ ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš°)"""
        try:
            print(f"ğŸ”§ ê¸°ë³¸ ë¶„ë¥˜ê¸°ë¡œ ì˜ˆì¸¡: '{text}'")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            self.classifier.eval()
            
            # ì…ë ¥ í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=512, 
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            with torch.no_grad():
                outputs = self.model(**inputs)
                # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                text_embedding = outputs.last_hidden_state[:, 0, :]
                # ë¶„ë¥˜ê¸°ë¡œ ì˜ˆì¸¡
                logits = self.classifier(text_embedding)
                probabilities = torch.softmax(logits, dim=-1)
                
                # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ
                predicted_class = torch.argmax(probabilities, dim=-1).item()
                confidence = probabilities[0][predicted_class].item()
            
            # ê¸°ë³¸ ì¶”ì²œ (ì—…ê³„ í‘œì¤€ ìš©ì–´)
            basic_recommendations = {
                "ì ê²°íƒ„": "ì½”í¬ìŠ¤",
                "ê´‘ì„": "ì² ê´‘ì„", 
                "ì •ë¦½ê´‘": "í ë¦¿",
                "ì„íšŒ": "ì„íšŒì„",
                "ì½”í¬ìŠ¤ ì˜¤ë¸ ì½”í¬ìŠ¤": "ì½”í¬ìŠ¤",
                "ì² ê´‘": "ì² ê´‘ì„",
                "Coke": "ì½”í¬ìŠ¤",
                "ì—´ìœ ì…": "ì—´ì—ë„ˆì§€",
                "ëƒ‰ê°ìˆ˜": "ëƒ‰ê°ìˆ˜",
                "ìœ¤í™œì œ": "ìœ¤í™œìœ ",
                "ì„ ì² ": "ì„ ì² ",
                "í™˜ì›ì² ": "í™˜ì›ì² ",
                "EAF íƒ„ì†Œ ì „ê·¹": "íƒ„ì†Œì „ê·¹",
                "ë¬¼": "ê³µì—…ìš©ìˆ˜",
                "ëª¨ë˜": "ì‚¬ì§ˆì¬",
                "ë¸”ë£¨": "ë¸”ë£¨",
                "ìš”ì†Œìˆ˜": "ìš”ì†Œ",
                "í˜•ê°•": "í˜•ê°•",
                "í¬ì¥ì¬": "í¬ì¥ì¬"
            }
            
            recommended = basic_recommendations.get(text, text)
            
            result = [{
                'rank': 1,
                'label': recommended,
                'similarity': confidence * 100
            }]
            
            print(f"âœ… ê¸°ë³¸ ë¶„ë¥˜ê¸° ì˜ˆì¸¡ ì™„ë£Œ: {text} â†’ {recommended} (ì‹ ë¢°ë„: {confidence*100:.1f}%)")
            return result
            
        except Exception as e:
            print(f"âŒ ê¸°ë³¸ ë¶„ë¥˜ê¸° ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return []
    
    def _cosine_similarity(self, vec1, vec2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0
        
        return dot_product / (norm1 * norm2)
    
    def update_with_feedback(self, text, correct_label, is_correct, memo=""):
        """ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        if is_correct:
            # ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ì´ì—ˆì„ ê²½ìš°, í•´ë‹¹ ë ˆì´ë¸”ì˜ ì„ë² ë”©ì„ ê°•í™”
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                text_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            # ê¸°ì¡´ ì„ë² ë”©ê³¼ ìƒˆë¡œìš´ ì„ë² ë”©ì„ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            if correct_label in self.label_embeddings:
                old_embedding = self.label_embeddings[correct_label]
                new_embedding = text_embedding[0]
                # ê°€ì¤‘ í‰ê·  (ê¸°ì¡´: 0.7, ìƒˆë¡œìš´: 0.3)
                self.label_embeddings[correct_label] = 0.7 * old_embedding + 0.3 * new_embedding
            else:
                self.label_embeddings[correct_label] = text_embedding[0]
        else:
            # í‹€ë¦° ì˜ˆì¸¡ì´ì—ˆì„ ê²½ìš°, ìƒˆë¡œìš´ ë ˆì´ë¸” ì¶”ê°€ ë˜ëŠ” ê¸°ì¡´ ë ˆì´ë¸” ì—…ë°ì´íŠ¸
            if correct_label not in self.label_to_id:
                # ìƒˆë¡œìš´ ë ˆì´ë¸” ì¶”ê°€
                new_id = len(self.label_to_id)
                self.label_to_id[correct_label] = new_id
                self.id_to_label[new_id] = correct_label
            
            # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¡œ í•´ë‹¹ ë ˆì´ë¸”ì˜ ì„ë² ë”© ìƒì„±
            inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                text_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            self.label_embeddings[correct_label] = text_embedding[0]
        
        # ë©”ëª¨ê°€ ìˆë‹¤ë©´ ì €ì¥ (í–¥í›„ ë”¥ëŸ¬ë‹ì„ ìœ„í•´)
        if memo:
            memo_file = Path(__file__).parent.parent.parent.parent / "data" / "studied" / "feedback_memos.jsonl"
            memo_file.parent.mkdir(exist_ok=True)
            
            memo_data = {
                'text': text,
                'correct_label': correct_label,
                'memo': memo,
                'timestamp': str(pd.Timestamp.now())
            }
            
            with open(memo_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(memo_data, ensure_ascii=False) + '\n')
    
    def load_training_data(self, model_dir):
        """í•™ìŠµëœ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # í•™ìŠµ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ìˆ˜ì •)
            # model_dir: service/datagather_service/app/data/studied/model_v24/model_v24
            # ëª©í‘œ: service/datagather_service/app/data/dataforstudy/file for trainning.jsonl
            
            # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°
            training_data_path = os.path.join(model_dir, "..", "..", "..", "dataforstudy", "file for trainning.jsonl")
            training_data_path = os.path.abspath(training_data_path)
            
            # ë””ë²„ê¹…: ê²½ë¡œ ì •ë³´ ì¶œë ¥
            print(f"ğŸ” ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_dir}")
            print(f"ğŸ” ê³„ì‚°ëœ í•™ìŠµ ë°ì´í„° ê²½ë¡œ: {training_data_path}")
            print(f"ğŸ” íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(training_data_path)}")
            
            # ëŒ€ì•ˆ ê²½ë¡œë“¤ë„ ì‹œë„ (ì ˆëŒ€ ê²½ë¡œ ê¸°ë°˜)
            current_dir = os.path.dirname(os.path.abspath(__file__))  # ananke í´ë”
            app_dir = os.path.dirname(current_dir)  # app í´ë”
            
            alternative_paths = [
                training_data_path,  # ì²« ë²ˆì§¸ ì‹œë„í•œ ê²½ë¡œ
                os.path.join(app_dir, "data", "dataforstudy", "file for trainning.jsonl"),  # app/data/dataforstudy/
                os.path.join(current_dir, "..", "data", "dataforstudy", "file for trainning.jsonl"),  # ananke/../data/dataforstudy/
                os.path.join(current_dir, "..", "..", "data", "dataforstudy", "file for trainning.jsonl"),  # ananke/../../data/dataforstudy/
            ]
            
            # ëŒ€ì•ˆ ê²½ë¡œë“¤ í™•ì¸
            for alt_path in alternative_paths:
                abs_alt_path = os.path.abspath(alt_path)
                print(f"ğŸ” ëŒ€ì•ˆ ê²½ë¡œ {alt_path}: {abs_alt_path} (ì¡´ì¬: {os.path.exists(abs_alt_path)})")
                if os.path.exists(abs_alt_path):
                    training_data_path = abs_alt_path
                    print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ë¡œ ë°œê²¬: {training_data_path}")
                    break
            
            if os.path.exists(training_data_path):
                print(f"ğŸ“š í•™ìŠµ ë°ì´í„° ë¡œë“œ ì‹œì‘: {training_data_path}")
                
                self.training_data = {}
                
                with open(training_data_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data = json.loads(line.strip())
                            label = data.get('Label', '')
                            
                            if label not in self.training_data:
                                self.training_data[label] = []
                            
                            # input_text 1~10ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì§‘
                            for i in range(1, 11):
                                input_text_key = f'input_text {i}'
                                if input_text_key in data and data[input_text_key]:
                                    self.training_data[label].append(data[input_text_key])
                
                print(f"âœ… í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.training_data)}ê°œ ë¼ë²¨, ì´ {sum(len(texts) for texts in self.training_data.values())}ê°œ input_text")
                
                # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
                for label, texts in list(self.training_data.items())[:3]:
                    print(f"  ğŸ“ {label}: {texts[:3]}...")
                    
                # ì²  ë¼ë²¨ ìƒì„¸ í™•ì¸
                if 'ì² ' in self.training_data:
                    print(f"ğŸ” ì²  ë¼ë²¨ ìƒì„¸ ì •ë³´:")
                    print(f"  ğŸ“ ì² : {self.training_data['ì² ']}")
                    print(f"  ğŸ“ ì²  ë¼ë²¨ì˜ ì´ input_text ìˆ˜: {len(self.training_data['ì² '])}")
                    
                    # ì² ã„¹ ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì, ê³µë°± ë¬´ì‹œ)
                    ì² ã„¹_í¬í•¨ = False
                    for i, input_text in enumerate(self.training_data['ì² ']):
                        if input_text and 'ì² ã„¹' in input_text:
                            ì² ã„¹_í¬í•¨ = True
                            print(f"  âœ… 'ì² ã„¹' ë°œê²¬! ìœ„ì¹˜: {i+1}ë²ˆì§¸, ê°’: '{input_text}'")
                    
                    if not ì² ã„¹_í¬í•¨:
                        print(f"  âŒ 'ì² ã„¹'ì´ ì²  ë¼ë²¨ì— ì—†ìŒ!")
                        # ì² ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  input_text ì¶œë ¥
                        ì² _ì‹œì‘ = [text for text in self.training_data['ì² '] if text and text.startswith('ì² ')]
                        print(f"  ğŸ” ì² ë¡œ ì‹œì‘í•˜ëŠ” input_textë“¤: {ì² _ì‹œì‘}")
                else:
                    print(f"âŒ 'ì² ' ë¼ë²¨ì´ training_dataì— ì—†ìŒ!")
                    
            else:
                print(f"âš ï¸ í•™ìŠµ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {training_data_path}")
                self.training_data = {}
                
        except Exception as e:
            print(f"âŒ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ğŸ” ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            self.training_data = {}
